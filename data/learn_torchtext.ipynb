{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext.data.utils  import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import io\n",
    "from typing import Iterable, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch                    2.0.0\n",
      "torch-tb-profiler        0.4.0\n",
      "torchaudio               0.13.0\n",
      "torchdata                0.6.0\n",
      "torchinfo                1.7.1\n",
      "torchmetrics             0.11.4\n",
      "torchnet                 0.0.4\n",
      "torchtext                0.15.1\n",
      "torchvision              0.15.1\n"
     ]
    }
   ],
   "source": [
    "!pip list|grep torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchtext.datasets import multi30k, Multi30k\n",
    "\n",
    "# # We need to modify the URLs for the dataset since the links to the original dataset are broken\n",
    "# # Refer to https://github.com/pytorch/text/issues/1756#issuecomment-1163664163 for more info\n",
    "# multi30k.URL[\"train\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/training.tar.gz\"\n",
    "# multi30k.URL[\"valid\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/validation.tar.gz\"\n",
    "\n",
    "# SRC_LANGUAGE = 'de'\n",
    "# TGT_LANGUAGE = 'en'\n",
    "\n",
    "# # Place-holders\n",
    "# token_transform = {}\n",
    "# vocab_transform = {}\n",
    "\n",
    "# token_transform[SRC_LANGUAGE] = get_tokenizer('spacy', language='de_core_news_md')\n",
    "# token_transform[TGT_LANGUAGE] = get_tokenizer('spacy', language='en_core_web_md')\n",
    "\n",
    "\n",
    "# # helper function to yield list of tokens\n",
    "# def yield_tokens(data_iter: Iterable, language: str) -> List[str]:\n",
    "#     language_index = {SRC_LANGUAGE: 0, TGT_LANGUAGE: 1}\n",
    "\n",
    "#     for data_sample in data_iter:\n",
    "#         yield token_transform[language](data_sample[language_index[language]])\n",
    "\n",
    "# # Define special symbols and indices\n",
    "# UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "# # Make sure the tokens are in order of their indices to properly insert them in vocab\n",
    "# special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
    "\n",
    "# for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "#     # Training data Iterator\n",
    "#     train_iter = Multi30k(root='/home/uih/JYL/GitHub/Transformer/data', split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "#     # Create torchtext's Vocab object\n",
    "#     vocab_transform[ln] = build_vocab_from_iterator(yield_tokens(train_iter, ln),\n",
    "#                                                     min_freq=1,\n",
    "#                                                     specials=special_symbols,\n",
    "#                                                     special_first=True)\n",
    "\n",
    "# # Set ``UNK_IDX`` as the default index. This index is returned when the token is not found.\n",
    "# # If not set, it throws ``RuntimeError`` when the queried token is not found in the Vocabulary.\n",
    "# for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "#   vocab_transform[ln].set_default_index(UNK_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in train_iter:\n",
    "#     print(i)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_LANGUAGE = 'zh'\n",
    "TGT_LANGUAGE = 'en'\n",
    "\n",
    "token_transform = {}\n",
    "vocab_transform = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_transform[SRC_LANGUAGE]= get_tokenizer('spacy', language='zh_core_web_md')\n",
    "token_transform[TGT_LANGUAGE]= get_tokenizer('spacy', language='en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_tokens(data_iter:Iterable, language:str) -> List[str]:\n",
    "    language_index = {SRC_LANGUAGE:0, TGT_LANGUAGE: 1}\n",
    "    for data_sample in data_iter:\n",
    "        yield token_transform[language](data_sample[language_index[language]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('加利福尼亚州水务工程的新问题', 'New Questions Over California Water Project')\n"
     ]
    }
   ],
   "source": [
    "def yield_source_pair(src_filepath, tgt_filepath):\n",
    "    raw_src_iter = io.open(src_filepath, encoding='utf8').readlines()\n",
    "    raw_tgt_iter = io.open(tgt_filepath, encoding='utf8').readlines()\n",
    "\n",
    "    for (raw_src, raw_tgt) in zip(raw_src_iter, raw_tgt_iter):\n",
    "        yield (raw_src.strip(), raw_tgt.strip())\n",
    "\n",
    "src_filepath = '/home/uih/JYL/GitHub/Transformer/data/dev_zh/newsdev2017-enzh-ref.zh.sgm.txt'\n",
    "tgt_filepath = '/home/uih/JYL/GitHub/Transformer/data/dev_en/newsdev2017-zhen-ref.en.sgm.txt'\n",
    "for p in yield_source_pair(src_filepath, tgt_filepath):\n",
    "    print(p)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define speical symbols and indices\n",
    "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "    train_iter = yield_source_pair(src_filepath, tgt_filepath)\n",
    "    vocab_transform[ln] = build_vocab_from_iterator(yield_tokens(train_iter, ln), min_freq=1, specials=special_symbols, special_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9681, 8917)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab_transform[SRC_LANGUAGE]), len(vocab_transform[TGT_LANGUAGE])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<unk>\n",
      "<pad>\n",
      "<bos>\n",
      "<eos>\n",
      "the\n",
      ",\n",
      ".\n",
      "of\n",
      "and\n",
      "to\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(vocab_transform[TGT_LANGUAGE])):\n",
    "    if i < 10:\n",
    "        print(vocab_transform[TGT_LANGUAGE].lookup_token(i))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set ``UNK_IDX`` as the default index. This index is returned when the token is not found. If not set, it throws ``RuntimeError`` when the queried token is not found in the Vocabulary.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "    vocab_transform[ln].set_default_index(UNK_IDX)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**As seen in the Data Sourcing and Processing section, our data iterator yields a pair of raw strings. \n",
    "We need to convert these string pairs into the batched tensors that can be processed by our Seq2Seq network defined previously. \n",
    "Below we define our collate function that converts a batch of raw strings into batch tensors that can be fed directly into our model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def sequentail_transforms(*transforms):\n",
    "    def func(txt_input):\n",
    "        for transform in transforms:\n",
    "            txt_input = transform(txt_input)\n",
    "        return txt_input\n",
    "    return func\n",
    "\n",
    "def tensor_transform(token_ids: List[int]):\n",
    "    return torch.cat((torch.tensor([BOS_IDX]), torch.tensor(token_ids), torch.tensor([EOS_IDX])))\n",
    "\n",
    "text_transform = {}\n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "    text_transform[ln] = sequentail_transforms(token_transform[ln], vocab_transform[ln], tensor_transform)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    src_batch, tgt_batch = [], []\n",
    "    print(batch)\n",
    "    for src_sample, tgt_sample in batch:\n",
    "        src_batch.append(text_transform[SRC_LANGUAGE](src_sample))\n",
    "        tgt_batch.append(text_transform[TGT_LANGUAGE](tgt_sample))\n",
    "    \n",
    "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n",
    "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX)\n",
    "    return src_batch.permute(1, 0), tgt_batch.permute(1, 0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**build dataloader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterator\n",
    "from torch.utils.data import IterableDataset\n",
    "\n",
    "class IterDataset(IterableDataset):  # 可迭代对象, 要实现__iter__方法(返回一个迭代器), 不能实现__next__方法\n",
    "\n",
    "    def __init__(self, src_filepath, tgt_filepath):\n",
    "        self.src_filepath = src_filepath\n",
    "        self.tgt_filepath = tgt_filepath\n",
    "\n",
    "    def __len__(self):\n",
    "        raw_src_iter = io.open(src_filepath, encoding='utf8').readlines()\n",
    "        raw_tgt_iter = io.open(tgt_filepath, encoding='utf8').readlines()\n",
    "        assert len(raw_src_iter) == len(raw_tgt_iter)\n",
    "        return len(raw_src_iter)\n",
    "    \n",
    "    def __iter__(self) -> Iterator:\n",
    "        return yield_source_pair(self.src_filepath, self.tgt_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE = 10\n",
    "train_iter = IterDataset(src_filepath, tgt_filepath)\n",
    "train_loader = DataLoader(train_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('加利福尼亚州水务工程的新问题', 'New Questions Over California Water Project'), ('在加利福尼亚州一个主要水务管理区披露州长杰瑞·布朗领导的行政当局将提供政府资金以完成两条巨型输水隧道的规划之后，有一些评论家和一位州议员表示，他们想进一步了解由谁来为州长所支持的拟耗资160亿美元的水务工程承担费用。', \"Critics and a state lawmaker say they want more explanations on who's paying for a proposed $16 billion water project backed by Gov. Jerry Brown, after a leading California water district said Brown's administration was offering government funding to finish the planning for the two giant water tunnels.\"), ('评论家表示，洛杉矶的MWD周四所提及的政府资金可能有悖于该州长期以来的承诺，该承诺说，为了实现布朗所设想的挖掘两条35英里长隧道，以便于把水从萨克拉门托河向南输送，并主要供应给加利福尼亚州中部和南部地区的愿景，各地方水管区（而非加利福尼亚州自身）将承担费用。', \"Critics said the government funding described by the Los Angeles-based Metropolitan Water District on Thursday could run counter to long-standing state assurances that various local water districts, not California itself, would pay for Brown's vision of digging twin 35-mile-long tunnels to carry water from the Sacramento River south, mainly for Central and Southern California.\"), ('这两条隧道的2.48亿美元初期费用支出尚未获得监管部门批准便已经成为一项正在进行的联邦审计的主题。', 'The $248 million in preliminary spending for the tunnels, which have yet to win regulatory approval, already is the topic of an ongoing federal audit.'), ('一些州议员也于周三责令对隧道费用支出情况进行一次州内审计。', 'On Wednesday, state lawmakers ordered a state audit of the tunnels-spending as well.'), ('该州的发言人南希·沃格尔周四表示，尽管该账户属于洛杉矶市政水管区，却不会动用该州的任何普通资金来完成这两条隧道当前规划阶段的工程。', \"On Thursday, state spokeswoman Nancy Vogel said that despite the account of the Los Angeles-based Metropolitan Water District, no money from the state's general fund would be used finishing the current planning phase of the twin tunnels.\"), ('不过，该隧道项目的反对者和一个纳税人团体周四提出批评，而且本周的审计命令幕后的州立法者之一州议会女议员苏珊·艾格曼则于周四要求该州政府澄清相关情况。', \"However, opponents of the tunnels and a taxpayer group were critical Thursday, and Assemblywoman Susan Eggman, one of the state lawmakers behind this week's audit order, asked the state Thursday for clarification.\"), ('霍华德贾维斯纳税人协会立法主任大卫·沃尔夫表示：“这是一场骗局。”', '\"It\\'s a shell game,\" said David Wolfe, the Howard Jarvis Taxpayers Association\\'s legislative director.'), ('我认为就昨天的审计（请求）来说：其中的问题远比答案多。', 'I think it comes back to the audit (request) yesterday: There are way more questions here than there are answers.'), ('该隧道项目受到布朗以及加利福尼亚州中部和南部一些具有政治影响力的供水管理区和用水客户的支持。', 'The tunnels project is endorsed by Brown and by some politically influential water districts and water customers in Central and Southern California.')]\n",
      "<bos> 评论家 表示 ， 洛杉矶 的 MWD 周四 所 提及 的 政府 资金 可能 有悖于 该州 长期 以来 的 承诺 ， 该 承诺 说 ， 为了 实现 布朗 所 设想 的 挖掘 两 条 35 英里 长 隧道 ， 以便 于 把 水 从 萨克拉门 托河 向 南 输送 ， 并 主要 供应 给 加利福尼亚州 中部 和 南部 地区 的 愿景 ， 各 地方 水管区 （ 而 非 加利福尼亚州 自身 ） 将 承担 费用 。 <eos> \n",
      "<bos> Critics said the government funding described by the Los Angeles - based Metropolitan Water District on Thursday could run counter to long - standing state assurances that various local water districts , not California itself , would pay for Brown 's vision of digging twin 35 - mile - long tunnels to carry water from the Sacramento River south , mainly for Central and Southern California . <eos> \n"
     ]
    }
   ],
   "source": [
    "def check_dataloader(pair, index=0):\n",
    "    assert index < pair[0].shape[0]\n",
    "    language_index = {SRC_LANGUAGE:0, TGT_LANGUAGE: 1}\n",
    "    ss_src, ss_tgt = \"\", \"\"\n",
    "    \n",
    "    for i in pair[language_index[SRC_LANGUAGE]][index]:\n",
    "        ss_src += vocab_transform[SRC_LANGUAGE].lookup_token(i) + \" \"\n",
    "\n",
    "    for i in pair[language_index[TGT_LANGUAGE]][index]:\n",
    "        ss_tgt += vocab_transform[TGT_LANGUAGE].lookup_token(i) + \" \"\n",
    "    print(ss_src, ss_tgt, sep='\\n')\n",
    "\n",
    "pair = next(iter(train_loader))\n",
    "check_dataloader(pair, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch1.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
